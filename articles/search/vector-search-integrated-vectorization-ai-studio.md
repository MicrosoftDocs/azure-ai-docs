---
title: Integrated Vectorization With Models From Microsoft Foundry
titleSuffix: Azure AI Search
description: Learn how to vectorize content during indexing in Azure AI Search with a Microsoft Foundry model.
author: gmndrg
ms.author: gimondra
ms.service: azure-ai-search
ms.custom:
  - build-2024
ms.topic: how-to
ms.date: 01/28/2026
---

# Use embedding models from the Microsoft Foundry model catalog for integrated vectorization

> [!IMPORTANT]
> This feature is in public preview under [Supplemental Terms of Use](https://azure.microsoft.com/support/legal/preview-supplemental-terms/). The latest preview version of [Skillsets - Create Or Update (REST API)](/rest/api/searchservice/skillsets/create-or-update) supports this feature.

In this article, you learn how to access embedding models from the [Microsoft Foundry model catalog](/azure/ai-foundry/how-to/model-catalog-overview) for vector conversions during indexing and query execution in Azure AI Search.

The workflow requires that you deploy a model from the catalog, which includes embedding models from Microsoft and other companies. Deploying a model is billable according to the billing structure of each provider.

After the model is deployed, you can use it with the [AML skill](cognitive-search-aml-skill.md) for integrated vectorization during indexing or with the [Microsoft Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) for queries.

> [!TIP]
> Use the [**Import data (new)** wizard](search-get-started-portal-import-vectors.md) to generate a skillset that includes an AML skill for deployed embedding models on Foundry. AML skill definition for inputs, outputs, and mappings are generated by the wizard, which gives you an easy way to test a model before writing any code.

## Prerequisites

+ An [Azure AI Search service](search-create-service-portal.md) in any region and on any pricing tier.

+ A [Microsoft Foundry hub-based project](/azure/ai-foundry/how-to/hub-create-projects).

## Supported embedding models

Supported embedding models from the model catalog vary by usage method:

+ For the latest list of models supported programmatically, see the [AML skill](cognitive-search-aml-skill.md) and [Microsoft Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) references.

+ For the latest list of models supported in the Azure portal, see [Quickstart: Vector search in the Azure portal](search-get-started-portal-import-vectors.md) and [Quickstart: Multimodal search in the Azure portal](search-get-started-portal-image-search.md).

## Deploy an embedding model from the model catalog

1. Follow [these portal instructions](/azure/ai-foundry/how-to/deploy-models-openai) to deploy a supported model to your project.

1. Make a note of the target URI, key, and model name. You need these values for the vectorizer definition in a search index and for the skillset that calls the model endpoints during indexing.

    If you prefer [token authentication](#connect-using-token-authentication) to key-based authentication, you only need to copy the model name. However, make a note of the region to which the model is deployed.

1. Configure a search index and indexer to use the deployed model.

   + To use the model during indexing, see [How to use integrated vectorization](vector-search-integrated-vectorization.md#how-to-use-integrated-vectorization). Be sure to use the [AML skill](cognitive-search-aml-skill.md), not the [Azure OpenAI Embedding skill](cognitive-search-skill-azure-openai-embedding.md). The next section describes the skill configuration.

   + To use the model as a vectorizer at query time, see [Configure a vectorizer](vector-search-how-to-configure-vectorizer.md). Be sure to use the [Microsoft Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) for this step.

<!--
1. Open the [Foundry model catalog](https://ai.azure.com/explore/models). Create a project if you don't have one already.

1. Apply a filter to show just the embedding models. Under **Inference tasks**, select **Embeddings**.

   :::image type="content" source="media\vector-search-integrated-vectorization-ai-studio\ai-studio-catalog-embeddings-filter.png" lightbox="media\vector-search-integrated-vectorization-ai-studio\ai-studio-catalog-embeddings-filter.png" alt-text="Screenshot of the Foundry model catalog page highlighting how to filter by embeddings models.":::

1. Select a supported model, and then select **Use this model**.

   :::image type="content" source="media\vector-search-integrated-vectorization-ai-studio\use-this-model.png" lightbox="media\vector-search-integrated-vectorization-ai-studio\use-this-model.png" alt-text="Screenshot of deploying a model via the Foundry model catalog.":::

1. Accept the defaults or modify as needed, and then select **Deploy**. The deployment details vary depending on which model you select.

1. Wait for the model to finish deploying by monitoring the **Provisioning State**. It should change from "Provisioning" to "Updating" to "Succeeded". You might need to select **Refresh** every few minutes to see the status update.

1. Make a note of the target URI, key, and model name. You need these values for the vectorizer definition in a search index, and for the skillset that calls the model endpoints during indexing.

    Optionally, you can change your endpoint to use **Token authentication** instead of **Key authentication**. If you enable token authentication, you only need to copy the URI and model name,  but make a note of which region the model is deployed to.

    :::image type="content" source="media\vector-search-integrated-vectorization-ai-studio\ai-studio-fields-to-copy.png" lightbox="media\vector-search-integrated-vectorization-ai-studio\ai-studio-fields-to-copy.png" alt-text="Screenshot of a deployed endpoint in Foundry portal highlighting the fields to copy and save for later.":::

1. You can now configure a search index and indexer to use the deployed model. 

   + To use the model during indexing, see [steps to enable integrated vectorization](vector-search-integrated-vectorization.md#how-to-use-integrated-vectorization). Be sure to use the [Azure Machine Learning (AML) skill](cognitive-search-aml-skill.md), and not the [AzureOpenAIEmbedding skill](cognitive-search-skill-azure-openai-embedding.md). The next section describes the skill configuration.

   + To use the model as a vectorizer at query time, see [Configure a vectorizer](vector-search-how-to-configure-vectorizer.md). Be sure to use the [Microsoft Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md) for this step.
-->

## Deploy an embedding model as a serverless deployment

The AML skill and Microsoft Foundry model catalog vectorizer only accept serverless deployments of Cohere embedding models. Serverless deployments via the Microsoft Foundry portal aren't supported for these models, so use the Azure CLI to create the deployment. For more information, see [Deploy models as serverless API deployments](/azure/ai-foundry/how-to/deploy-models-serverless).

To create a Cohere serverless deployment:

1. Install the Azure CLI with the `ml` extension.

    ```azurecli
    az extension add -n ml
    ```

1. Sign in to Azure and set your defaults.

    ```azurecli
    az login
    az account set --subscription <subscription-id>
    az configure --defaults workspace=<project-name> group=<resource-group>
    ```

1. Create a `subscribe.yaml` file to subscribe to the marketplace subscription for the model.

    ```yaml
    name: cohere-embed-v3-english-subscription
    model_id: azureml://registries/azureml-cohere/models/Cohere-embed-v3-english
    ```

    For other supported models, replace the model ID with one of the following values.
    
    | Model | Model ID |
    | ----- | -------- |
    | Cohere-embed-v3-english | `azureml://registries/azureml-cohere/models/Cohere-embed-v3-english` |
    | Cohere-embed-v3-multilingual | `azureml://registries/azureml-cohere/models/Cohere-embed-v3-multilingual` |
    | Cohere-embed-v4 | `azureml://registries/azureml-cohere/models/Cohere-embed-v4` |

1. Run the following command to create the subscription.

    ```azurecli
    az ml marketplace-subscription create --file subscribe.yaml
    ```

1. Create an `endpoint.yaml` file to create the serverless endpoint.

    ```yaml
    name: cohere-embed-v3-english-endpoint
    model_id: azureml://registries/azureml-cohere/models/Cohere-embed-v3-english
    ```

1. Run the following command to create the endpoint.

    ```azurecli
    az ml serverless-endpoint create --file endpoint.yaml
    ```

1. For key-based authentication, get the endpoint URI and key for use in the skill or vectorizer.

    ```azurecli
    az ml serverless-endpoint show --name cohere-embed-v3-english-endpoint --query "scoring_uri"
    az ml serverless-endpoint get-credentials --name cohere-embed-v3-english-endpoint
    ```

## Sample AML skill payload

When you deploy embedding models from the model catalog, you connect to them using the [AML skill](cognitive-search-aml-skill.md) in Azure AI Search for indexing workloads.

This section describes the AML skill definition and index mappings. It includes a sample payload that's already configured to work with its corresponding deployed endpoint. For more information, see [Skill context and input annotation language](cognitive-search-skill-annotation-language.md).

<!-- ### [**Text Input for "Inference" API**](#tab/inference-text)

This AML skill payload works with the following models from Foundry:

+ Cohere-embed-v3-english
+ Cohere-embed-v3-multilingual

It assumes that you're chunking your content using the [Text Split skill](cognitive-search-skill-textsplit.md) and that the text to be vectorized is in the `/document/pages/*` path. If your text comes from a different path, update all references to the `/document/pages/*` path accordingly.

The URI and key are generated when you deploy the model from the catalog. For more information about these values, see [How to deploy large language models with Foundry](/azure/ai-foundry/how-to/deploy-models-open).

```json
{
  "@odata.type": "#Microsoft.Skills.Custom.AmlSkill",
  "context": "/document/pages/*",
  "uri": "<YOUR_MODEL_URL_HERE>",
  "key": "<YOUR_MODEL_KEY_HERE>",
  "inputs": [
    {
      "name": "input_data",
      "sourceContext": "/document/pages/*",
      "inputs": [
        {
          "name": "columns",
          "source": "=['image', 'text']"
        },
        {
          "name": "index",
          "source": "=[0]"
        },
        {
          "name": "data",
          "source": "=[['', $(/document/pages/*)]]"
        }
      ]
    }
  ],
  "outputs": [
    {
      "name": "text_features"
    }
  ]
}
``` -->

<!--
### [**Facebook embedding models**](#tab/inference-image)

This AML skill payload works with the following image embedding models from Foundry:

+ Facebook-DinoV2-Image-Embeddings-ViT-Base
+ Facebook-DinoV2-Image-Embeddings-ViT-Giant

It assumes that your images come from the `/document/normalized_images/*` path that is created by enabling [built in image extraction](cognitive-search-concept-image-scenarios.md). If your images come from a different path or are stored as URLs, update all references to the `/document/normalized_images/*` path according.

The URI and key are generated when you deploy the model from the catalog. For more information about these values, see [Add and configure models to Azure AI model inference](/azure/ai-foundry/model-inference/how-to/create-model-deployments).

```json
{
  "@odata.type": "#Microsoft.Skills.Custom.AmlSkill",
  "context": "/document/normalized_images/*",
  "uri": "https://myproject-1a1a-abcd.eastus.inference.ml.azure.com/score",
  "timeout": "PT1M",
  "key": "bbbbbbbb-1c1c-2d2d-3e3e-444444444444",
  "inputs": [
    {
      "name": "input_data",
      "sourceContext": "/document/normalized_images/*",
      "inputs": [
        {
          "name": "columns",
          "source": "=['image', 'text']"
        },
        {
          "name": "index",
          "source": "=[0]"
        },
        {
          "name": "data",
          "source": "=[[$(/document/normalized_images/*/data), '']]"
        }
      ]
    }
  ],
  "outputs": [
    {
      "name": "image_features"
    }
  ]
}
```

### [**Cohere embedding models**](#tab/cohere)
-->

### Cohere embedding models

This AML skill payload works with the following embedding models:

+ Cohere-embed-v3-english
+ Cohere-embed-v3-multilingual
+ Cohere-embed-v4

It assumes that you're chunking your content using the Text Split skill and therefore your text to be vectorized is in the `/document/pages/*` path. If your text comes from a different path, update all references to the `/document/pages/*` path accordingly.

You must add the `/v1/embed` path onto the end of the URL that you copied from your Foundry deployment. You might also change the values for the `input_type`, `truncate`, and `embedding_types` inputs to better fit your use case. For more information on the available options, review the [Cohere Embed API reference](/azure/ai-foundry/how-to/deploy-models-cohere-embed).

The URI and key are generated when you deploy the model from the catalog. For more information about these values, see [How to deploy Cohere Embed models with Foundry](/azure/ai-foundry/how-to/deploy-models-cohere-embed).

```json
{
  "@odata.type": "#Microsoft.Skills.Custom.AmlSkill",
  "context": "/document/pages/*",
  "uri": "https://cohere-embed-v3-multilingual-hin.eastus.models.ai.azure.com/v1/embed",
  "key": "aaaaaaaa-0b0b-1c1c-2d2d-333333333333",
  "inputs": [
    {
      "name": "texts",
      "source": "=[$(/document/pages/*)]"
    },
    {
      "name": "input_type",
      "source": "='search_document'"
    },
    {
      "name": "truncate",
      "source": "='NONE'"
    },
    {
      "name": "embedding_types",
      "source": "=['float']"
    }
  ],
  "outputs": [
    {
      "name": "embeddings",
      "targetName": "aml_vector_data"
    }
  ]
}
```

In addition, the output of the Cohere model isn't the embeddings array directly, but rather a JSON object that contains it. You need to select it appropriately when mapping it to the index definition via `indexProjections` or `outputFieldMappings`. Here's a sample `indexProjections` payload that would allow you to do implement this mapping.

If you selected a different `embedding_types` in your skill definition, change `float` in the `source` path to the type you selected.

```json
"indexProjections": {
  "selectors": [
    {
      "targetIndexName": "<YOUR_TARGET_INDEX_NAME_HERE>",
      "parentKeyFieldName": "ParentKey", // Change this to the name of the field in your index definition where the parent key will be stored
      "sourceContext": "/document/pages/*",
      "mappings": [
        {
          "name": "aml_vector", // Change this to the name of the field in your index definition where the Cohere embedding will be stored
          "source": "/document/pages/*/aml_vector_data/float/0"
        }
      ]
    }
  ],
  "parameters": {}
}
```

## Sample vectorizer payload

The [Microsoft Foundry model catalog vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md), unlike the AML skill, is tailored to work only with embedding models that are deployable via the model catalog. The main difference is that you don't have to worry about the request and response payload. However, you must provide the `modelName`, which corresponds to the "Model ID" that you copied after deploying the model.

Here's a sample payload of how you would configure the vectorizer on your index definition given the properties copied from Foundry.

For Cohere models, you should NOT add the `/v1/embed` path to the end of your URL like you did with the skill.

```json
"vectorizers": [
    {
        "name": "<YOUR_VECTORIZER_NAME_HERE>",
        "kind": "aml",
        "amlParameters": {
            "uri": "<YOUR_URL_HERE>",
            "key": "<YOUR_PRIMARY_KEY_HERE>",
            "modelName": "<YOUR_MODEL_ID_HERE>"
        },
    }
]
```

## Connect using token authentication

If you can't use key-based authentication, you can configure the AML skill and Microsoft Foundry model catalog vectorizer connection for [token authentication](../machine-learning/how-to-authenticate-online-endpoint.md) via role-based access control on Azure.

Your search service must have a [system or user-assigned managed identity](search-how-to-managed-identities.md), and the identity must have **Owner** or **Contributor** permissions for your project. You can then remove the `key` field from your skill and vectorizer definition, replacing it with `resourceId`. If your project and search service are in different regions, also provide the `region` field.

```json
"uri": "<YOUR_URL_HERE>",
"resourceId": "subscriptions/<YOUR_SUBSCRIPTION_ID_HERE>/resourceGroups/<YOUR_RESOURCE_GROUP_NAME_HERE>/providers/Microsoft.MachineLearningServices/workspaces/<YOUR_AML_WORKSPACE_NAME_HERE>/onlineendpoints/<YOUR_AML_ENDPOINT_NAME_HERE>",
"region": "westus", // Only needed if project is in different region from search service
```

> [!NOTE]
> This integration doesn't currently support token authentication for Cohere models. You must use key-based authentication.

## Related content

+ [Configure a vectorizer in a search index](vector-search-how-to-configure-vectorizer.md)
+ [Configure index projections in a skillset](index-projections-concept-intro.md)
+ [AML skill](cognitive-search-aml-skill.md)
+ [Foundry vectorizer](vector-search-vectorizer-azure-machine-learning-ai-studio-catalog.md)
+ [Skill context and input annotation language](cognitive-search-skill-annotation-language.md)
