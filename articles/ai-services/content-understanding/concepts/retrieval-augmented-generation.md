---
title: Azure AI Content Understanding retrieval-augmented generation
titleSuffix: Azure AI services
description: Learn about retrieval-augmented generation
author: laujan
ms.author: tonyeiyalla
manager: nitinme
ms.service: azure-ai-content-understanding
ms.topic: overview
ms.date: 04/25/2025
---

# Retrieval-augmented generation with Content Understanding

Retrieval-augmented Generation (**RAG**) is a method that enhances the capabilities of Large Language Models (**LLM**) by integrating data from external knowledge sources. Integrating diverse and current information refines the precision and contextual relevance of the outputs generated by an **LLM**. A key challenge for **RAG** is the efficient extraction and processing of multimodal content—such as documents, images, audio, and video—to ensure accurate retrieval and effective use to bolster the **LLM** responses.

Azure AI Content Understanding addresses these challenges by offering advanced content extraction capabilities across diverse modalities. The service seamlessly integrates advanced natural language processing, computer vision, and speech recognition into a unified framework. This integration eliminates the complexities of managing separate extraction pipelines and workflows. A unified approach ensures superior data handling for documents, images, audio, and video, thus enhancing both precision and depth in information retrieval. Such innovation proves especially beneficial for **RAG** applications, where the accuracy and contextual relevance of responses depend on a deep understanding of interconnections, interrelationships, and context.

:::image type="content" source="../media/concepts/rag-architecture-1.png" alt-text="screenshot of Azure Content Understanding service architecture." lightbox="../media/concepts/rag-architecture-1.png" :::

## Multimodal data and RAG

In traditional content processing, simple text extraction sufficed for many content processing use cases. Modern enterprise environments encompass a vast array of complex information across diverse formats:

 * **Documents** featuring intricate layouts.
 * **Images** rich with visual details and insights.
 * **Audio** recordings capturing pivotal conversations.
 * **Videos** that seamlessly integrate and unify multiple data types.

For **RAG** systems to deliver truly comprehensive outputs, all content must be accurately processed and made accessible to generative AI models. This method guarantees that users receive relevant answers to their queries, regardless of the original format of the information. The **RAG** system excels in retrieving and maintaining relevance across diverse scenarios: it seamlessly processes detailed tables from financial reports, interprets complex technical diagrams from manuals, extracts insights from recorded conference calls, and efficiently manages explanations presented in training videos.

## Effective RAG using Content Understanding

Context-aware chunking is key to optimizing **RAG** with multimodal content. It breaks large content or datasets into smaller, manageable parts for better processing and retrieval. However, different types of data present unique challenges:

* **Documents**. Layout and meaning must be preserved to avoid losing context.
* **Images**. Visual elements need accurate interpretation while maintaining their relationships.
* **Audio**. Speaker identification and time order are important to keep the narrative clear.
* **Video**: Scene boundaries and synchronization between modes must stay intact.

Semantic chunking prioritizes preserving meaning and relationships rather than instituting arbitrary splits. By ensuring that retrieved chunks align closely with queries, chunking enables more precise and coherent responses. Azure AI Content Understanding is designed for multimodal RAG applications and processes diverse content types (documents, images, audio, and video) while preserving context and meaning. Chunking also improves query relevance and downstream processing operations, making it and ideal solutions for enterprise use cases that demand deep, in-depth content analysis and understanding.

## Content Understanding RAG capabilities

Azure AI Content Understanding addresses the core challenges of multimodal **RAG**—complex data ingestion, data representation, and query optimization—by providing a solution that enhances the accuracy and relevance of retrieval and generation processes:

* **Simplified multimodal ingestion:** Content Understanding streamlines the processing of various content types—documents, images, audio, and video—into a cohesive and unified workflow. Preserving structural integrity and contextual relationships eliminates the complexities of handling multimodal data, ensuring consistent representation across all modalities.

* **Enhanced data representation:** Content Understanding transforms unstructured data into structured, context-rich formats like Markdown and JSON. This structured transformation facilitates seamless integration with embedding models, vector databases, and generative AI systems. Maintaining semantic depth, hierarchical structure, and cross-modal linkages, addresses issues like semantic fragmentation and enables more accurate information retrieval.

* **Customizable field extraction:** Users can define custom fields to generate targeted metadata, such as summaries, visual descriptions, or sentiment analysis. These tailored outputs enrich knowledge bases with domain-specific insights, enhancing standard content extraction and vector representations. The result is improved retrieval accuracy and more contextually relevant responses.

* **Optimized query performance:** Content Understanding mitigates modality bias and context fragmentation by providing structured, enriched data that supports advanced relevance ranking across modalities. This approach ensures that user queries yield the most relevant information, enhancing the coherence and precision of generated responses.

:::image type="content" source="../media/concepts/rag-architecture-2.png" alt-text="Screenshot of Content Understanding RAG architecture overview, process, and workflow with Azure AI Search and Azure OpenAI." lightbox="../media/concepts/rag-architecture-2.png" :::

Content extraction forms the foundation of effective RAG systems by transforming raw multimodal data into structured, searchable formats optimized for retrieval. The implementation varies by content type:
- **Document:** Extracts hierarchical structures, such as headers, paragraphs, tables, and page elements, preserving the logical organization of training materials.
- **Audio:** Generates speaker-aware transcriptions that accurately capture spoken content while automatically detecting and processing multiple languages.
- **Video:** Divides video into meaningful units, transcribes spoken content, and provides scene descriptions while addressing context window limitations in generative AI models.

While content extraction provides a strong foundation for indexing and retrieval, it may not fully address domain-specific needs or provide deeper contextual insights. Learn more about [content extraction](./capabilities.md#content-extraction) capabilities.

1. [Extract content](#content-extraction). Convert unstructured multimodal data into a structured representation.

   Field extraction complements content extraction by generating targeted metadata that enriches the knowledge base and improves retrieval precision. The implementation varies by content type:

   * **Document:** Extract key topics/fields to provide concise overviews of lengthy materials.
   * **Image:** Convert visual information into searchable text by verbalizing diagrams, extracting embedded text, and identifying graphical components.
   * **Audio:** Extract key topics or sentiment analysis from conversations and to provide added context for queries.
   * **Video:** Generate scene-level summaries, identify key topics, or analyze brand presence and product associations within video footage.

1. [Create a unified search index](#create-a-unified-search-index). Store the embedded vectors in a database or search index for efficient retrieval.

Learn more about [field extraction](./capabilities.md#field-extraction) capabilities.

###  Content extraction

The RAG implementation process starts with data extraction using Azure AI Content Understanding. This step establishes the groundwork for converting raw multimodal data into structured, searchable formats tailored for RAG workflows. Content extraction is ideal for transforming raw multimodal data into structured, searchable formats:

* **Documents:** Captures hierarchical structures, like headers, paragraphs, tables, and page elements, preserving the logical organization of training materials remains intact.
* **Images**: Transforms visual data into searchable text by verbalizing diagrams and charts, extracting embedded text, and converting graphical data into structured formats. Technical illustrations are analyzed to identify components and relationships.
* **Audio:** Generates speaker-aware transcriptions that accurately capture spoken content across multiple languages through automatic detection and processing.
* **Video:** Segments video content into meaningful units using scene detection and key frame extraction. It creates descriptive summaries, transcribes spoken dialogue, identifies key topics, and analyzes sentiment indicators throughout the footage. Scene descriptions are provided while addressing context limitations inherent to generative AI models.

#### Field extraction

While content extraction provides a strong foundation for indexing and retrieval, it may not fully address specialized domain-specific requirements or deliver deeper contextual insights. Field extraction is a valuable complement to content extraction by producing targeted metadata that enriches the knowledge base and improves retrieval accuracy:

* **Document:** Summarizes key topics or fields to provide clear overviews of extensive materials.
* **Image:** Transforms visual content into searchable text by interpreting diagrams, extracting embedded text, and recognizing graphical elements.
* **Audio:** Analyzes conversations to extract key topics, assess sentiment, and offer more context for inquiries.
* **Video:** Creates scene-level summaries, identifies main topics, and examines brand presence or product associations in video content.


Integrating content extraction with field extraction enables organizations to develop a knowledge base that is context-rich and optimized for indexing, retrieval, and RAG scenarios. This approach enables more precise and relevant responses to user inquiries. To learn more, *see* [content extraction](./capabilities.md#content-extraction) and [field extraction](./capabilities.md#field-extraction) capabilities.

#### Code sample: analyzer and schema configuration

The following code samples show an analyzer and schema creation for various modalities in a multimodal RAG scenario.

---

# [Document](#tab/document)
```json
{
  "description": "Training document analyzer",
  "scenario": "document",
  "config": {
    "returnDetails": true
  },
  "fieldSchema": {
    "fields": {
      "ChapterTitle": {
        "type": "string",
        "method": "extract",
        "description": "Training chapter title"
      },
      "ChapterAuthor": {
        "type": "string",
        "method": "extract",
        "description": "Training chapter author"
      },
      "ChapterPublishDate": {
        "type": "Date",
        "method": "extract",
        "description": "Training chapter publication date"
      }
    }
  }
}
```

# [Image](#tab/image)
```json
{
  "description": "Training images analyzer",
  "scenario": "image",
  "fieldSchema": {
    "fields": {
      "TrainingChartTitle": {
        "type": "string",
        "method": "extract",
        "description": "Training chart title or caption"
      },
      "TrainingChartType": {
        "type": "string",
        "method": "classify",
        "enum": [ "bar", "line", "pie" ]
      },
      "TrainingChartDescription": {
        "type": "string",
        "method": "extract",
        "description": "Training chart description"
      }
    }
  }
}
```

# [Audio](#tab/audio)
```json
{
  "description": "Training audio analyzer",
  "scenario": "audio",
  "config": {
    "returnDetails": true,
    "locales": ["en-US"]
  },
  "fieldSchema": {
    "fields": {
      "TrainingSummary": {
        "type": "string",
        "method": "generate",
        "description": "detailed summary of discussion in this segment "
      },
      "TrainingTopics": {
        "type": "array",
        "method": "generate",
        "description": "2-3 topics of discussion in this segment "
      },
      "People": {
        "type": "array",
        "description": "List of people mentioned",
        "items": {
          "type": "object",
          "properties": {
            "Name": { "type": "string" },
            "Role": { "type": "string" }
          }
        }
      }
    }
  }
}
```

# [Video](#tab/video)
```json
{
  "description": "Training video analyzer",
  "scenario": "video",
  "fieldSchema": {
    "fields": {
      "Description": {
        "type": "string",
        "method": "generate",
        "description": "Detailed summary of the video segment, focusing on product characteristics, and quality inspection requirements"
      },
      "KeyTopics": {
        "type": "array",
        "method": "generate",
        "description": "The key points or topics covered in this segment"
      }
    }
  }
}
```


---

#### Code sample: extraction response

The following code sample showcases the results of content and field extraction using Azure AI Content Understanding. These results demonstrate how multimodal data is transformed into structured, enriched formats, ready for indexing and retrieval in RAG workflows.

---

# [Document](#tab/document)

```json
{
  "id": "bcf8c7c7-03ab-4204-b22c-2b34203ef5db",
  "status": "Succeeded",
  "result": {
    "analyzerId": "training_document_analyzer",
    "apiVersion": "2024-12-01-preview",
    "createdAt": "2024-11-13T07:15:46Z",
    "warnings": [],
    "contents": [
      {
        "markdown": "CONTOSO LTD.\n\n\n# Contoso Training Topics\n\nContoso Headquarters...",
        "fields": {
          "ChapterTitle": {
            "type": "string",
            "valueString": "Risks and Compliance regulations",
            "spans": [ { "offset": 0, "length": 12 } ],
            "confidence": 0.941,
            "source": "D(1,0.5729,0.6582,2.3353,0.6582,2.3353,0.8957,0.5729,0.8957)"
          },
          "ChapterAuthor": {
            "type": "string",
            "valueString": "John Smith",
            "spans": [ { "offset": 0, "length": 12 } ],
            "confidence": 0.941,
            "source": "D(1,0.5729,0.6582,2.3353,0.6582,2.3353,0.8957,0.5729,0.8957)"
          },
          "ChapterPublishDate": {
            "type": "Date",
            "valueString": "04-11-2017",
            "spans": [ { "offset": 0, "length": 12 } ],
            "confidence": 0.941,
            "source": "D(1,0.5729,0.6582,2.3353,0.6582,2.3353,0.8957,0.5729,0.8957)"
          },
        },
        "kind": "document",
        "startPageNumber": 1,
        "endPageNumber": 1,
        "unit": "inch",
        "pages": [
          {
            "pageNumber": 1,
            "angle": -0.0039,
            "width": 8.5,
            "height": 11,
            "spans": [ { "offset": 0, "length": 1650 } ],
            "words": [
              {
               ....
              },
            ],
            "lines": [
              {
                ...
              },
            ]
          }
        ],

      }
    ]
  }
}
```

# [Image](#tab/image)

```json
{
  "id": "12fd421b-b545-4d63-93a5-01284081bbe1",
  "status": "Succeeded",
  "result": {
    "analyzerId": "training_image_analyzer",
    "apiVersion": "2024-12-01-preview",
    "createdAt": "2024-11-09T08:41:00Z",
    "warnings": [],
    "contents": [
      {
        "markdown": "![image](image)\n",
        "fields": {
          "TrainingChartTitle": {
            "type": "string",
            "valueString": "Weekly Work Hours Distribution"
          },
          "TrainingChartType": {
            "type": "string",
            "valueString": "pie"
          },
          "TrainingChartDescription"{
            "type": "string",
            "valueString": "This chart shows the monthly sales data for the year 2025."
          }
        },
        "kind": "document",
        "startPageNumber": 1,
        "endPageNumber": 1,
        "unit": "pixel",
        "pages": [
          {
            "pageNumber": 1,
            "width": 1283,
            "height": 617
          }
        ]
      }
    ]
  }
}
```

# [Audio](#tab/audio)

```json
{
  "id": "247c369c-1aa5-4f92-b033-a8e4318e1c02",
  "status": "Succeeded",
  "result": {
    "analyzerId": "training_audio_analyzer",
    "apiVersion": "2024-12-01-preview",
    "createdAt": "2024-11-09T08:42:58Z",
    "warnings": [],
    "contents": [
      {
        "kind": "audioVisual",
        "startTimeMs": 0,
        "endTimeMs": 32182,
        "markdown": "```WEBVTT\n\n00:00.080 --> 00:00.640\n<v Agent>Good day...",
        "fields": {
          "TrainingSummary": {
            "type": "string",
            "valueString": "Maria Smith contacted Contoso to inquire about her current point balance. Agent John Doe confirmed her identity and informed her that she has 599 points. Maria did not require any further information and the call ended on a positive note."
          },
          "TrainingTopics": {
                        "type": "array",
                        "valueArray": [
                            {
                                "type": "string",
                                "valueString": "Compliance"
                            },
                            {
                                "type": "string",
                                "valueString": "Risk mitigation"
                            },]
          },
          "People": {
            "type": "array",
            "valueArray": [
              {
                "type": "object",
                "valueObject": {
                  "Name": {
                    "type": "string",
                    "valueString": "Maria Smith"
                  },
                  "Role": {
                    "type": "string",
                    "valueString": "Customer"
                  }
                }
              }, ...
            ]
          }
        },
        "transcriptPhrases": [
          {
            "speaker": "Agent 1",
            "startTimeMs": 80,
            "endTimeMs": 640,
            "text": "Good day.",
            "confidence": 0.932,
            "words": [
              {
                "startTimeMs": 80,
                "endTimeMs": 280,
                "text": "Good"
              }, ...
            ],
            "locale": "en-US"
          }, ...
        ]
      }
    ]
  }
}
```

# [Video](#tab/video)

```json
{
  "id": "204fb777-e961-4d6d-a6b1-6e02c773d72c",
  "status": "Succeeded",
  "result": {
    "analyzerId": "sample_video_analyzer",
    "apiVersion": "2024-12-01-preview",
    "createdAt": "2024-11-09T08:57:21Z",
    "warnings": [],
    "contents": [
      {
        "kind": "audioVisual",
        "startTimeMs": 0,
        "endTimeMs": 2800,
        "width": 540,
        "height": 960,
        "markdown": "# Shot 0:0.0 => 0:1.800\n\n## Transcript\n\n```\n\nWEBVTT\n\n0:0.80 --> 0:10.560\n<v Speaker>When I was planning my trip...",
        "fields": {

          "description": {
            "type": "string",
            "valueString": "The video begins with a view from a glass floor, showing a person's feet in white sneakers standing on it. The scene captures a downward view of a structure, possibly a tower, with a grid pattern on the floor and a clear view of the ground below. The lighting is bright, suggesting a sunny day, and the colors are dominated by the orange of the structure and the gray of the floor."
          },
          "KeyTopics": {
                        "type": "array",
                        "valueArray": [
                            {
                                "type": "string",
                                "valueString": "Flight delay"
                            },
                            {
                                "type": "string",
                                "valueString": "Customer service"
                            },
            ]
          }
        },
      ...
    ]
  }
}
```

---

After data is extracted using Azure AI Content Understanding, the next steps involve integrating it with Azure AI Search and Azure OpenAI. This integration demonstrates the seamless synergy between data extraction, retrieval, and generative AI, creating a comprehensive and efficient solution for RAG scenarios.

> [!div class="nextstepaction"]
> [View full code sample for Multimodal RAG on GitHub.](https://github.com/Azure-Samples/azure-ai-search-with-content-understanding-python/blob/main/notebooks/search_with_multimodal_RAG.ipynb)

### Create a unified search index

After Azure AI Content Understanding processes multimodal content, the next essential step is to develop a powerful search framework that effectively uses the enriched structured data. You can use [Azure OpenAI's embedding models](../../openai/how-to/embeddings.md) to embed markdown and JSON outputs. By indexing these embeddings with [Azure AI Search](https://docs.azure.cn/en-us/search/tutorial-rag-build-solution-index-schema), you can create an integrated knowledge repository. This repository effortlessly bridges various content modalities.

Azure AI Search provides advanced search strategies to maximize the value of multimodal content.

In this implementation, [hybrid search](../../../search/hybrid-search-overview.md) combines vector and full-text indexing to blend keyword precision with semantic understanding—ideal for complex queries requiring both exact matching and contextual relevance. By carefully selecting and configuring these search techniques based on your specific use case requirements, you can ensure that your RAG system retrieves the most relevant content across all modalities, significantly enhancing the quality and accuracy of generated responses.

The following JSON code sample shows a minimal consolidated index that support vector and hybrid search and enables cross-modal search capabilities, allowing users to discover relevant information regardless of the original content format:

```json
{
"name": "unified_training_index",
"fields": [
    # Document content fields
    {"name": "document_content", "type": "Edm.String", "searchable": true, "retrievable": true},
    {"name": "document_headers", "type": "Edm.String", "searchable": true, "retrievable": true},

    # Image-derived content
    {"name": "visual_descriptions", "type": "Edm.String", "searchable": true, "retrievable": true},
    { "name": "chunked_content_vectorized", "type": "Edm.Single", "dimensions": 1536, "vectorSearchProfile": "my-vector-profile", "searchable": true, "retrievable": false, "stored": false },

    # Video content components
    {"name": "video_transcript", "type": "Edm.String", "searchable": true, "retrievable": true},
    {"name": "scene_descriptions", "type": "Edm.String", "searchable": true, "retrievable": true},
    {"name": "video_topics", "type": "Edm.String", "searchable": true, "retrievable": true},
    { "name": "chunked_content_vectorized", "type": "Edm.Single", "dimensions": 1536, "vectorSearchProfile": "my-vector-profile", "searchable": true, "retrievable": false, "stored": false },

    # Audio processing results
    {"name": "audio_transcript", "type": "Edm.String", "searchable": true, "retrievable": true},
    {"name": "speaker_attribution", "type": "Edm.String", "searchable": true, "retrievable": true},
    {"name": "conversation_topics", "type": "Edm.String", "searchable": true, "retrievable": true}
],

  "vectorSearch": {
      "algorithms": [
          { "name": "my-algo-config", "kind": "hnsw", "hnswParameters": { }  }
      ],
      "profiles": [
        { "name": "my-vector-profile", "algorithm": "my-algo-config" }
      ]
  }
}
```
---

### Utilize Azure OpenAI models

Once your content is extracted and indexed, integrate [Azure OpenAI's embedding and chat models](../../openai/concepts/models.md) to create an interactive question-answering system:

This approach grounds the response with your actual content, enabling the model to answer questions by referencing specific document sections, describing relevant images, quoting from video transcripts, or citing speaker statements from audio recordings.

The combination of Content Understanding's extraction capabilities, Azure AI Search's retrieval functions, and Azure OpenAI's generation abilities creates a powerful end-to-end RAG solution that can seamlessly work with all your content types.

## Get started

Content Understanding supports the following development options:

* [REST API](../quickstart/use-rest-api.md) Quickstart.

* [Azure Foundry](../quickstart//use-ai-foundry.md) Portal Quickstart.

## Next steps

* Try our [RAG code samples.](https://github.com/Azure-Samples/azure-ai-search-with-content-understanding-python#samples)

* Follow our [RAG Tutorial](../tutorial/build-rag-solution.md)

* Learn more about [document](../document/overview.md), [image](../image/overview.md), [audio](../audio/overview.md), and [video](../video/overview.md) capabilities

* Learn more about Content Understanding [**best practices**](../concepts/best-practices.md) and [**capabilities**](../concepts/capabilities.md)

* Review Content Understanding [**code samples**](https://github.com/Azure-Samples/azure-ai-search-with-content-understanding-python?tab=readme-ov-file#azure-ai-search-with-content-understanding-samples-python)
