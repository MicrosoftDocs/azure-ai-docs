### YamlMime:FAQ
metadata:
  title: Text to speech frequently asked questions (FAQ)
  titleSuffix: Foundry Tools
  description: Get answers to frequently asked questions about the text to speech service.
  author: PatrickFarley
  manager: nitinme
  ms.service: azure-ai-speech
  ms.topic: faq
  ms.date: 08/07/2025
  ms.author: pafarley
title: Text to speech FAQ
summary: |
  This article answers commonly asked questions about the text to speech (TTS) capability. If you can't find answers to your questions here, check out [other support options](../cognitive-services-support-options.md?context=%2fazure%2fcognitive-services%2fspeech-service%2fcontext%2fcontext%253fcontext%253d%2fazure%2fcognitive-services%2fspeech-service%2fcontext%2fcontext).
  

sections:
  - name: General
    questions:
      - question: |
          How does the billing work for text to speech?
        answer: |
          Text to speech usage is billed per character. Check the definition of billable characters in the [pricing note](text-to-speech.md#pricing-note).
      - question: |
          What is the rate limit for the text to speech synthesis requests?
        answer: |
          The text to speech synthesis rate scales automatically as it receives more requests. A default rate limit is set per speech resource. The rate is adjustable with business justifications and no extra charges are incurred for rate limit increase. Check more details in [Speech service quotas and limits](speech-services-quotas-and-limits.md#text-to-speech-quotas-and-limits-per-resource).
      - question: |
          How would we disclose to the end user that the voice is a synthetic voice?
        answer: |
          We recommend that every user should follow our [code of conduct](/azure/ai-foundry/responsible-use-of-ai-overview) when using the text to speech capability. There are several ways to disclose the synthetic nature of the voice including implicit and explicit byline. Refer to [Disclosure design guidelines](/azure/ai-foundry/responsible-ai/speech-service/text-to-speech/concepts-disclosure-guidelines).
      - question: |
          How can I reduce the latency for my voice app?
        answer: |
          We provide several tips for you to lower the latency and bring the best performance to your users. See [Lower speech synthesis latency using Speech SDK](how-to-lower-speech-synthesis-latency.md).
      - question: |
          What output audio formats does text to speech support?
        answer: |
          Azure AI text to speech supports various streaming and non-streaming audio formats, with the commonly used sampling rates. All TTS standard voices are created to support high-fidelity audio outputs with 48 kHz and 24 kHz. The audio can be resampled to support other rates as needed. See [Audio outputs](rest-text-to-speech.md#audio-outputs).
      - question: |
          Can the voice be customized to stress specific words?
        answer: |
          Adjusting the emphasis is supported for some voices depending on the locale. See the [emphasis tag](speech-synthesis-markup-voice.md#adjust-emphasis).
      - question: |
          Can we have multiple strength for each emotion, like sad, slightly sad, and so on, in?
        answer: |
          Adjusting the style degree is supported for some voices depending on the locale. See the [mstts:express-as tag](speech-synthesis-markup-voice.md#use-speaking-styles-and-roles).
      - question: |
          Is there a mapping between Viseme IDs and mouth shape?
        answer: |
          Yes. See [Get facial position with viseme](how-to-speech-synthesis-viseme.md?tabs=visemeid#map-phonemes-to-visemes).
      - question: |
          Why am I getting HTTP 429 (Too Many Requests) errors when using Text-to-Speech Standard Voice?
        answer: |
          The default quota of 200 transactions per second (TPS) for Text-to-Speech (TTS) Standard Voice is designed to accommodate the needs of most customers. In most cases, HTTP 429 errors are not caused by quota limitations, but by insufficient backend service capacity in the selected region for the specified voice. Increasing the quota does not resolve these capacity constraints. To address this issue effectively:
          - Use native regions: Deploy the voice in a region where it is natively supported and better resourced (for example, use the Japan region for Japanese voices).
          - Select popular voices: Choose a more commonly used voice within your current region to reduce the likelihood of hitting capacity limits.

  - name: Audio Content Creation
    questions:
        - question: |
            How can I reference a lexicon file that I created on the Audio Content Creation platform in my code?
          answer: |
            First, you can open the lexicon file on the Audio Content Creation and obtain the lexicon file ID, which is located before "?fileKind=CustomLexiconFile" in the file path. For example, if the file path is `https://speech.microsoft.com/portal/d391a094f76846acbcd11dc2ba835f4f/audiocontentcreation/file/6cbc2527-8d57-4c1b-b9d9-3ea6d13ca95c?fileKind=CustomLexiconFile`, the lexicon file ID is `6cbc2527-8d57-4c1b-b9d9-3ea6d13ca95c`. Then, switch a file referencing this lexicon to SSML format on the Audio Content Creation. In the SSML file, locate the `<!--ID=FCB` xml node, where you can find the URI of the lexicon file based on the mentioned file ID. Finally, reference the lexicon file URI link using the SSML lexicon element in your code. For instance, if you locate the XML node `<!--ID=FCB5B6FB566-33CA-4B68-BEAF-B013C53B3368;Version=1|{"Files":{"6cbc2527-8d57-4c1b-b9d9-3ea6d13ca95c":{"FileKind":"CustomLexiconFile","FileSubKind":"CustomLexiconFile","Uri":"https://cvoiceprodwus2.blob.core.windows.net/acc-public-files/d391a094f76846acbcd11dc2ba835f4f/e9a6a5a2-9cef-47f4-b961-d175be75d92f.xml"}}}`, you can obtain the lexicon file URI `https://cvoiceprodwus2.blob.core.windows.net/acc-public-files/d391a094f76846acbcd11dc2ba835f4f/e9a6a5a2-9cef-47f4-b961-d175be75d92f.xml`.
        
  - name: Professional voice fine-tuning
    questions:
      - question: |
          How much data is required for professional voice fine-tuning?
        answer: |
          You need training data of at least 300 lines of recordings (or approximately 30 minutes of speech) for professional voice fine-tuning. We recommend 2,000 lines of recordings (or approximately 2-3 hours of speech) to create a voice for production use. For the script selection criteria, see [Record custom voice samples](record-custom-voice-samples.md).
      - question: |
          Can we include duplicate text sentences in the same set of training data?
        answer: |
          No. The service will flag the duplicate sentences and just keep the first imported one. For the script selection criteria, see [Record custom voice samples](record-custom-voice-samples.md).
      - question: |
          Can we include multiple styles in the same set of training data?
        answer: |
          We recommend that you keep the style consistent in one set of training data. If the styles are different, put them into different training sets. In this case, consider using the multi-style training method of professional voice fine-tuning. For the script selection criteria, see [Record custom voice samples](record-custom-voice-samples.md).
      - question: |
          Does switching styles via SSML work for custom voices?
        answer: |
          Switching styles via SSML works for both multi-style standard voices and multi-style custom voices. With multi-style training, you can create a voice that speaks in different styles, and you can also adjust these styles via SSML.
      - question: |
          How does cross-lingual voice work with languages that have different pronunciation structure and assembly?  
        answer: |
          Sentence structure and pronunciation naturally vary across languages such as English and Japanese. Each neural voice is trained with audio data recorded by native speaking voice talent. For [cross lingual](professional-voice-train-voice.md?tabs=crosslingual#train-your-custom-voice-model) voice, we transfer the major features like timbre to sound like the original speaker and preserve the right pronunciation. For example, a cross-lingual voice uses the native way to speak Japanese and still sounds similar (but not exactly) like the original English speaker. 
      - question: |
          Can I use professional voice fine-tuning to customize pronunciation for my domain?
        answer: |
          Professional voice fine-tuning enables you to create a brand voice for your business. You can optimize it for your domain as well. We recommend you include domain-specific samples in your training data for higher naturalness. However, the pronunciation is defined by the Speech service by default. We don't support pronunciation customization with professional voice fine-tuning. If you want to customize pronunciation for your voice, use SSML. See [Pronunciation with Speech Synthesis Markup Language (SSML)](speech-synthesis-markup-pronunciation.md).
      - question: |
          After one training can I train my voice again? 
        answer: |
          You can train again. Each training creates a new voice model. You are charged for each training. 
      - question: |
          Is the model version the same as the engine version?
        answer: |
          No. The model version is different from the engine version. The model version means the version of the training recipe for your model and varies by the features supported and model training time. Foundry Tools text to speech engines are updated from time to time to capture the latest language model that defines the pronunciation of the language. After you've trained your voice, you can apply your voice to the new language model by updating to the latest engine version. When a new engine is available, you're prompted to update your neural voice model. See [Update engine version for your voice model](professional-voice-train-voice.md?tabs=neural#update-engine-version-for-your-voice-model).
      - question: |
          Can we limit the number of trainings using Azure Policy or other features? Or is there any way to avoid false training?
        answer: |
          If you want to limit the permission to training, you can limit the user roles and access. Refer to [Role-based access control for Speech resources](role-based-access-control.md).
      - question: |
          Ca Microsoft add a mechanism to prevent unauthorized use or misuse of our voice when it's created?
        answer: |
          The voice model can only be used by yourselves using your own token. Microsoft also doesn't use your data. See [Data, privacy, and security](/azure/ai-foundry/responsible-ai/speech-service/text-to-speech/data-privacy-security). You can also request to add watermarks to your voice to protect your model. See [Microsoft Azure Neural TTS introduces the watermark algorithm for synthetic voice identification](https://techcommunity.microsoft.com/t5/ai-cognitive-services-blog/introducing-the-watermark-algorithm-for-synthetic-voice/ba-p/3298548).
      - question: |
          Do you have any tips about contracts or negotiation with voice actors?
        answer: |
          We have no recommendations on contracts and it's up to the customer and the voice talent to negotiate the terms. However, you should make sure the voice talent understands the capabilities of text to speech, including its potential risks, and provide explicit consent to create a synthetic version of their voice in both the contract and a verbal statement. See [Disclosure for voice talent](/azure/ai-foundry/responsible-ai/speech-service/text-to-speech/disclosure-voice-talent). 
      - question: |
          Do we need to return the written permission from the voice talent back to Microsoft?
        answer: |
          Microsoft doesn't need the written permission, but you must obtain consent from your voice talent. The voice talent will also be required to record the consent statement and it must be uploaded into Speech Studio before training can begin. See [Set up voice talent for professional voice fine-tuning](professional-voice-create-consent.md).



      
additionalContent: |

  ## Next steps
  
  - [Text to speech quickstart](get-started-text-to-speech.md)
  - [What's new](releasenotes.md)

