---
title: Responsible AI for Microsoft Foundry
titleSuffix: Microsoft Foundry
description: Learn how to use Foundry Tools and features responsibly with Microsoft Foundry.
manager: nitinme
keywords: Foundry Tools, cognitive
ms.service: azure-ai-foundry
ms.topic: overview
ms.date: 02/13/2026
ms.author: pafarley
author: PatrickFarley
ms.custom: ignite-2024
monikerRange: 'foundry-classic || foundry'
---

# Responsible AI for Microsoft Foundry

[!INCLUDE [version-banner](includes/version-banner.md)]

This article provides an overview of the resources for building and deploying trustworthy AI agents. This includes end-to end security, observability, and governance with controls and checkpoints at all stages of the agent lifecycle. Our recommended essential development steps are grounded in the [Microsoft Responsible AI Standard](https://aka.ms/RAI), which sets policy requirements that our own engineering teams follow. Much of the content of the Standard follows a pattern, asking teams to Discover, Protect, and Govern potential content risks.

At Microsoft, our approach is guided by a governance framework rooted in AI principles, which establish product requirements and serve as our "north star." When we identify a business use case for generative AI, we first discover and assess the potential risks of the AI system to pinpoint critical focus areas.

Once we identify these risks, we evaluate their prevalence within the AI system  through systematic measurement, helping us prioritize areas that need attention. We then apply appropriate protection at the model and agent level against those risks.

Finally, we examine strategies for managing risks in production, including deployment and operational readiness and setting up monitoring to support ongoing governance to ensure compliance and surface new risks after the application is live.

In alignment with Microsoft's RAI practices, these recommendations are organized into three stages:

* **Discover** agent quality, safety, and security risks before and after deployment. For example, test your agent with adversarial prompts to identify potential jailbreak vulnerabilities.
* **Protect** – at both the model output and agent runtime levels – against security risks, undesirable outputs, and unsafe actions. Use content filters and guardrails to block harmful outputs before they reach users.
* **Govern** agents through tracing and monitoring tools and compliance integrations. Set up continuous monitoring to track agent behavior and detect anomalies in production.

## View and respond to security alerts

You can view Defender for Cloud security alerts and recommendations to improve your security posture in the **Risks + alerts** section. Security alerts are the notifications generated by Defender for Foundry Tools plan when threats are identified in your AI workloads. You can take action in Azure portal or in the Defender portal to address these alerts.

To view security alerts:

1. Sign in to the [Microsoft Foundry portal](https://ai.azure.com).
2. Navigate to your project.
3. In the left navigation, select **Risks + alerts**.
4. Review active alerts and recommendations.
5. Select an alert to see details and remediation steps.

* To learn more about security alerts, see [Alerts for AI workloads (Preview)](/azure/defender-for-cloud/alerts-ai-workloads).
* To learn more about security recommendations, see [Review security recommendations](/azure/defender-for-cloud/review-security-recommendations).


## Learn more about responsible AI

- [Microsoft AI principles](https://www.microsoft.com/ai/responsible-ai)
- [Microsoft responsible AI resources](https://www.microsoft.com/ai/tools-practices)
- [Microsoft Azure Learning courses on responsible AI](/ai)
