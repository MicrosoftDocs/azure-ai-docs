---
title: "Intervention points concepts"
titleSuffix: Azure AI services
description: Learn about Agentic AI intervention points.
services: ai-services
author: PatrickFarley
manager: nitinme
ms.service: azure-ai-content-safety
ms.topic: quickstart
ms.date: 09/16/2025
ms.author: ssalgado
zone_pivot_groups: programming-languages-content-safety-foundry-rest
monikerRange: 'foundry-classic || foundry'
---

# Intervention points

Agentic AI expands both capability and attack surface. As soon as an agent can call external tools, write to databases, or trigger downstream processes, malfunctions or malicious attacks can lead to steering it off course, leaking sensitive data, or executing harmful actions. Relying solely on guardrails applied to models can leave these vectors exposed. To close this gap Azure AI Foundry allows guardrails to be applied directly to agents and allows the individual controls within those guardrails to be applied to four different intervention points:

| Intervention Point | Description | Example Control at this Intervention Point |
|-------------------|-------------|-------------------------------------------|
| User input | A query sent from a user to a model or agent. Sometimes referred to as "prompt." Some controls at this intervention point require the inclusion of document embedding by the user to take effect. | **Risk:** User input attacks<br>**Action:** Annotate and block<br><br>When this control is specified in an agent's or model's guardrail, the user's input is scanned by a classification model that detects jailbreak attacks. If an attack is detected, the user's input is blocked from being sent to the model, halting the model. |
| Tool call | The next action the agent is proposing to take, as generated by its underlying model. The tool call consists of which tool is called and the arguments it is called with, including data being sent to the tool. | **Risk:** Task Adherence<br>**Action:** Annotate and block<br><br>When this control is specified, every time the agent is about to execute a tool call, the proposed tool call is scanned by the Task Adherence API. If the tool call is misaligned to the user's input and the agent's purpose, the tool call is executed and the agent stops functioning until it receives another user input. |
| Tool response | The content sent back by a tool, internal to an agent's orchestration and before the content is to the agent's memory or given back to the end user. | **Risk:** Indirect attack<br>**Action:** Annotate and block<br><br>When this control is specified, the full payload sent back from each tool to this agent is scanned for attempted indirect prompt injection attacks. If detected, the agent stops operating immediately, preventing the malicious content from being saved by the agent and from maliciously steering the agent off-track. |
| Output | The final content sent back to the end user in response to their query. | **Risk:** Protected Material for Text<br>**Action:** Annotate only<br><br>When this control is specified, the final content meant to be displayed to the user is scanned for certain types of copyrighted text. If detected, there is a flag in the annotation response for the API used to call this model or agent. |

> [!IMPORTANT]
> Only certain types of tools are subject to controls at the tool call and tool response intervention points. Currently, OpenAI tools including FileSearch and CodeInterpreter, as well as custom tools, are not subject to controls. All tools added as Connections to your Foundry project and then added to an agent are subject to controls. 
