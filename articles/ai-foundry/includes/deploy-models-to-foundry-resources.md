---
title: Include file
description: Include file
ms.author: mopeakande
author: msakande
ms.service: azure-ai-foundry
ms.subservice: azure-ai-foundry-model-inference
ms.topic: include
ms.date: 06/13/2025
ms.custom: include
---

> [!NOTE]
> We recommend that you deploy Microsoft Foundry Models to **Foundry resources** so that you can consume your deployments in the resource via a single endpoint with the same authentication and schema to generate inference. The endpoint follows the [Azure AI Model Inference API](/rest/api/aifoundry/modelinference/) which all the Foundry Models support. To learn how to deploy a Foundry Model to the Foundry resources, see [Add and configure models to Foundry Models](../foundry-models/how-to/create-model-deployments.md).