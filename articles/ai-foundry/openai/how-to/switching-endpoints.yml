### YamlMime:HowTo

metadata:
  title: How to switch between OpenAI and Azure OpenAI endpoints with Python
  titleSuffix: Azure OpenAI Service
  description: Learn about the changes you need to make to your code to swap back and forth between OpenAI and Azure OpenAI endpoints.
  author: mrbullwinkle
  ms.author: mbullwin
  manager: nitinme
  ms.date: 09/30/2025
  ms.service: azure-ai-foundry
  ms.subservice: azure-ai-foundry-openai
  ms.topic: how-to
  ms.custom:
    - devx-track-python
    - ge-structured-content-pilot
title: |
  How to switch between OpenAI and Azure OpenAI endpoints with Python
introduction: | 
  While OpenAI and Azure OpenAI rely on a [common Python client library](https://github.com/openai/openai-python), there are small changes you need to make to your code in order to swap back and forth between endpoints. This article walks you through the common changes and differences you'll experience when working across OpenAI and Azure OpenAI.

procedureSection:
  - title: |
      Authentication
    summary: |
      We recommend using Microsoft Entra ID or Azure Key Vault. You can use environment variables for testing outside of your production environment.     
      ### API key
    code: |
      <table>
      <tr>
      <td> OpenAI </td> <td> Azure OpenAI </td>
      </tr>
      <tr>
      <td>

      ```python
      import os
      from openai import OpenAI

      client = OpenAI(
          api_key=os.getenv("OPENAI_API_KEY")
      )



      ```

      </td>
      <td>

      ```python
      import os
      from openai import OpenAI
          
      client = OpenAI(
          api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
          base_url = "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/"
      )
      ```

      </td>
      </tr>
      </table>



      ### Microsoft Entra ID authentication

      <table>
      <tr>
      <td> OpenAI </td> <td> Azure OpenAI </td>
      </tr>
      <tr>
      <td>

      ```python
      import os
      from openai import OpenAI

      client = OpenAI(
          api_key=os.getenv("OPENAI_API_KEY")
      )








      ```

      </td>
      <td>

      ```python
      from azure.identity import DefaultAzureCredential, get_bearer_token_provider
      from openai import OpenAI

      token_provider = get_bearer_token_provider(
          DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
      )

      client = OpenAI(
          base_url = "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/", 
          api_key = token_provider,
      )
      ```

      </td>
      </tr>
      </table>

  - title: |
      Keyword argument for model
    summary: |
      OpenAI uses the `model` keyword argument to specify what model to use. Azure OpenAI has the concept of unique model [deployments](create-resource.md?pivots=web-portal#deploy-a-model). When you use Azure OpenAI, `model` should refer to the underlying deployment name you chose when you deployed the model.

      > [!IMPORTANT]
      > When you access the model via the API in Azure OpenAI, you need to refer to the deployment name rather than the underlying model name in API calls, which is one of the [key differences](../how-to/switching-endpoints.yml) between OpenAI and Azure OpenAI. OpenAI only requires the model name. Azure OpenAI always requires deployment name, even when using the model parameter. In our docs, we often have examples where deployment names are represented as identical to model names to help indicate which model works with a particular API endpoint. Ultimately your deployment names can follow whatever naming convention is best for your use case.
    code: |
      <table>
      <tr>
      <td> OpenAI </td> <td> Azure OpenAI </td>
      </tr>
      <tr>
      <td>

      ```python
      response = client.responses.create(   
          model="gpt-4.1-nano", # Replace with your model deployment name 
          input="This is a test."
      )

      chat_completion = client.chat.completions.create(
          model="gpt-4o",
          messages="<messages>"
      )

      embedding = client.embeddings.create(
          model="text-embedding-3-large",
          input="<input>"
      )
      ```

      </td>
      <td>

      ```python
      response = client.responses.create(   
          model="gpt-4.1-nano", # Replace with your model deployment name 
          input="This is a test."
      )

      chat_completion = client.chat.completions.create(
          model="gpt-4o", # model = "deployment_name".
          messages="<messages>"
      )

      embedding = client.embeddings.create(
          model="text-embedding-3-large", # model = "deployment_name".
          input="<input>"
      )
      ```

      </td>
      </tr>
      </table>

  - title: |
      Azure OpenAI embeddings multiple input support
    summary: |
      OpenAI and Azure OpenAI currently support input arrays up to 2,048 input items for text-embedding-ada-002. Both require the max input token limit per API request to remain under 8,191 for this model.
    code: |
      <table>
      <tr>
      <td> OpenAI </td> <td> Azure OpenAI </td>
      </tr>
      <tr>
      <td>

      ```python
      inputs = ["A", "B", "C"] 

      embedding = client.embeddings.create(
          input=inputs,
          model="text-embedding-3-large"
      )


      ```

      </td>
      <td>

      ```python
      inputs = ["A", "B", "C"] #max array size=2048

      embedding = client.embeddings.create(
          input=inputs,
          model="text-embedding-3-large" # This must match the custom deployment name you chose for your model.
          # engine="text-embedding-ada-002"
      )

      ```

      </td>
      </tr>
      </table>

relatedContent:
  - text: Learn more about how to work the Responses API with our how-to guide
    url: ../how-to/responses.md
