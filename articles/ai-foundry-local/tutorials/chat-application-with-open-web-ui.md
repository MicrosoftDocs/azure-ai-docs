---
title: Build a Chat application with Open Web UI
titleSuffix: Foundry Local
description: Learn how to build a chat application with FOundry local and Open Web UI
manager: scottpolly
keywords: Azure AI services, cognitive, AI models, local inference
ms.service: azure-ai-foundry
ms.topic: tutorial
ms.date: 02/20/2025
ms.reviewer: samkemp
ms.author: samkemp
author: samuel100
ms.custom: build-2025
#customer intent: As a developer, I want to get started with Foundry Local so that I can run AI models locally.
---

# Build a Chat application with Open Web UI

This tutorial guides you through setting up a chat application using Foundry Local and Open Web UI. By the end, you'll have a fully functional chat interface running locally on your device.

## Prerequisites

Before beginning this tutorial, make sure you have:

- **Foundry Local** [installed](../get-started.md) on your machine.
- **At least one model loaded** using the `foundry model load` command, for example:
  ```bash
  foundry model load Phi-4-mini-gpu-int4-rtn-block-32
  ```

## Set up Open Web UI for chat

1. **Install Open Web UI** by following the installation instructions from the [Open Web UI github](https://github.com/open-webui/open-webui).

2. **Start Open Web UI** by running the following command in your terminal:
   ```bash
   open-webui serve
   ```

Then open your browser and navigate to [http://localhost:8080](http://localhost:8080).

3. **Connect Open Web UI to Foundry Local**:

   - Go to **Settings** in the navigation menu
   - Select **Connections**
   - Choose **Manage Direct Connections**
   - Click the **+** icon to add a new connection
   - For the URL, enter `http://localhost:5272/v1`
   - For the API Key, it presently can't be blank, so you can enter any value (e.g. `test`)
   - Save the connection

4. **Start chatting with your model**:
   - The model list should automatically populate at the top of the UI
   - Select one of your loaded models from the dropdown
   - Begin your chat in the input box at the bottom of the screen

That's it! You're now chatting with your AI model running completely locally on your device.

## Next steps

- Try [different models](../how-to/manage.md) to compare performance and capabilities
